{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMu2bZY2JxxmsUg6Z+PdNqE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dev-chaitanya-dewangan/CHATGPT_TRANSFORM_FROM_SCRATCH/blob/main/LLM_FROM_SCRATCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREP"
      ],
      "metadata": {
        "id": "TRzYCvG0OJZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ehBrOX-MEha"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "url = \"https://www.gutenberg.org/cache/epub/76137/pg76137.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Get the text content\n",
        "full_text = response.text\n",
        "first_index = full_text.find(\"PREFACE\")\n",
        "second_index = full_text.find(\"PREFACE\", first_index + 1)\n",
        "# Preview first 500 characters\n",
        "book= full_text[second_index:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preproccesed_cleaned_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)',book)\n",
        "preporcessed_result=[item.strip() for item in preproccesed_cleaned_text if item.strip()]"
      ],
      "metadata": {
        "id": "LW2SFb8qiN9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EXTENED WITH UNK AND ENDOF TEXT**"
      ],
      "metadata": {
        "id": "4ld_NlhPiPKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preporcessed_sorted_result=sorted(set(preporcessed_result))\n",
        "preporcessed_sorted_result.extend([\"<|endoftext|>\",\"<|unk|>\"])"
      ],
      "metadata": {
        "id": "gUkOSLJjiON5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab={token:id for id,token in enumerate(preporcessed_sorted_result)}\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JLNH4wQQO7-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOKENIZER\n"
      ],
      "metadata": {
        "id": "jmKNB_m0YnAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self,vocab):\n",
        "    self.encoded_vocab=vocab\n",
        "    self.decoded_vocab={id:word for word,id in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    text_cleaning=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preporccesed_text=[item.strip() for item in text_cleaning if item.strip()]\n",
        "    tokenized_text = [self.encoded_vocab[i] for i in preporccesed_text]\n",
        "    return tokenized_text\n",
        "  def decode(self,tokens):\n",
        "    text=\" \".join([self.decoded_vocab[i] for i in tokens])\n",
        "    text=re.sub('\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "kgs5MmMwO_Jc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEW TOKENIZER EXTENDED WITH THE UNKOWN TEXT AND END OF TEXT\\"
      ],
      "metadata": {
        "id": "dyi2_RrFjlRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TokenizerV2:\n",
        "  def __init__(self,vocab):\n",
        "    self.encoded_vocab=vocab\n",
        "    self.decoded_vocab={id:word for word,id in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    text_cleaning=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preporccesed_text=[item.strip() for item in text_cleaning if item.strip()]\n",
        "    # tokenized_text = [self.encoded_vocab[i] for i in preporccesed_text] CHANGED HERE\n",
        "    tokenized_text = [i if i in self.encoded_vocab else \"<|unk|>\" for i in preporccesed_text]\n",
        "    return tokenized_text\n",
        "  def decode(self,tokens):\n",
        "    text=\" \".join([self.decoded_vocab[i] for i in tokens])\n",
        "    text=re.sub('\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "bLNvdygVO_MS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TokenizerV2(vocab)\n"
      ],
      "metadata": {
        "id": "r5_E2-50v4co"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "class GPTEncoder(Dataset):\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.input=[]\n",
        "    self.target=[]\n",
        "    tokenized_text=tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
        "    for i in range(0,len(tokenized_text)-max_length,stride):\n",
        "      input=tokenized_text[i:i+max_length]\n",
        "      target=tokenized_text[i+1:1+i+max_length]\n",
        "      self.input.append(torch.tensor(input))\n",
        "      self.target.append(torch.tensor(target))\n",
        "  def __len__(self):\n",
        "    return len(self.input)\n",
        "  def __getitem__(self,pos):\n",
        "    return self.input[pos],self.target[pos]\n",
        "\n",
        "def create_dataloader(txt,batch_size=4,max_length=256\n",
        "                      ,stride=128,shuffle=True\n",
        "                      ,drop_last=True,num_workers=0):\n",
        "  tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTEncoder(txt,tokenizer,max_length,stride)\n",
        "  dataloader=DataLoader(\n",
        "      dataset=dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "  return dataloader\n",
        "book= full_text[second_index:]\n",
        "batch_dataloader=create_dataloader(book,batch_size=1,max_length=12,stride=120,shuffle=False)\n",
        "data_iter=iter(batch_dataloader)\n",
        "# first=next(batch_dataloader)\n",
        "print(next(data_iter))"
      ],
      "metadata": {
        "id": "uZxmxh_tO_Oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ea845da-9bd2-4e07-d397-620edfd5f20a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[   47, 31688, 11598,    13,   201,   198,   201,   198,   201,   198,\n",
            "           464,  1204]]), tensor([[31688, 11598,    13,   201,   198,   201,   198,   201,   198,   464,\n",
            "          1204,   286]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SIMPLIFIED SELF ATTENTION**"
      ],
      "metadata": {
        "id": "mQNgJ7O4wmHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "attenS=torch.empty(inputs.shape[0])\n",
        "query = inputs[1]\n",
        "for i,x_i in  enumerate(inputs):\n",
        "  attenS[i]=torch.dot(x_i,query)\n",
        "\n",
        "print(attenS)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "966PA7Fauzfc",
        "outputId": "fcabe639-8358-4626-871f-408b6a1ee352"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attenS=torch.softmax(attenS,dim=0)\n",
        "print(attenS.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlz9V8uPwdHe",
        "outputId": "db83203c-e7e0-4e4b-ea87-f3f3c5deb82e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.0000)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[3,5,9], # Your     (x^1)\n",
        "   [5,7,6], # journey  (x^2)\n",
        "   [7,5,4], # starts   (x^3)\n",
        "   [2,8,3], # with     (x^4)\n",
        "   [7,5,0], # one      (x^5)\n",
        "   [5,0,5]] # step     (x^6)\n",
        ",dtype=torch.float32)\n",
        "\n",
        "atten_scores=inputs @ inputs.T\n",
        "attention_weights=torch.softmax(atten_scores,dim=-1)\n",
        "context=attention_weights @ atten_scores\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8CPLX5X6O_Re",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72a80cfd-790b-497f-d765-ee666c3d8b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([6, 6])"
            ]
          },
          "metadata": {},
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FULL CHATGPT MODEL IMPLEMENTED**"
      ],
      "metadata": {
        "id": "eDFX03NRdvEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#CONFIG FOR GPT2 144M\n",
        "GPT2_CONFIG_144M = {\n",
        "\n",
        "\t\"vocab_size\":50257,\n",
        "\t\"n_heads\":12\n",
        "\t\"n_layers\":12,\n",
        "\t\"emb_dim\":768,\n",
        "\t\"context_length\":1024,\n",
        "\t\"drop_rate\":0.1,\n",
        "\t\"ff_dim\" :3072\n",
        "}\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class LayerNorm(nn.module):\n",
        "\tdef __init__(self,dim,eps=1e-5):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.wieghts =nn.Parameter(torch.ones(dim))\n",
        "\t\tself.bias =nn.Parameter(torch.zeros(dim))\n",
        "\t\tself.eps=eps\n",
        "\tdef forward(self,x):\n",
        "\t\tmean   =x.mean(-1,keepdim=True)\n",
        "\t\tvar    =x.var(-1,keepdim=True,unbiased=False)\n",
        "\t\tx_norm =(x-mean)/torch.sqrt(var+self.eps)\n",
        "\t\treturn self.weights*x_norm+self.bias\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "\tdef __init__(self,cfg):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.net=nn.Sequential(\n",
        "\t\t\tnn.Linear(cfg['emb_dim'],cfg['ff_dim']),\n",
        "\t\t\tnn.GELU(),\n",
        "\t\t\tnn.Linear(cfg['ff_dim'],cfg['emb_dim']),\n",
        "\t\t\tnn.Dropout(cfg['drop_rate'])\n",
        "\t\t\t)\n",
        "\tdef forward(self,x):\n",
        "\t\treturn self.net(x)\n",
        "class MultiHeadAttention(nn.Module):\n",
        "\tdef __init__(self,cfg):\n",
        "\t\tself.n_heads=cfg['n_heads']\n",
        "\t\tself.emb_dim=cfg['emb_dim']\n",
        "\t\tself.head_dim=self.emb_dim/self.n_heads\n",
        "\t\tassert self.head_dim*self.n_heads == self.emb_dim,\n",
        "\n",
        "\t\tself.context_input=nn.Linear(self.emb_dim,3*self.emb_dim)\n",
        "\t\tself.output       =nn.Linear(self.emb_dim,self.emb_dim)\n",
        "\t\tself.dropout      =nn.Droptout(cfg['dropt_rate'])\n",
        "\n",
        "\t\tself.register_buffer(\"mask\",torch.tril(torch.ones(cfg[\"context_length\"],cfg[\"context_length\"])).unsqueeze(0).unsqueeze(0))\n",
        "\tdef forward(self,x):\n",
        "\t\tB,T,C=x.size()\n",
        "\n",
        "\t\tcontext_input=self.context_input(x)\n",
        "\t\tcontext_input=context_input.reshape(B,T,3,self.n_head,self.head_dim)\n",
        "\t\tcontext_input=context_input.permute(2,0,3,1,4)\n",
        "\t\tq,k,v=context_input[0],context_input[1],context_input[2]\n",
        "\n",
        "\t\tattn_scores=(q@k.transpose(-2,-1)/(self.head_dim**o.5))\n",
        "\t\tattn_probs =F.softmax(attn_scores,dim=-1)\n",
        "\t\tattn_probs =self.dropout(attn_probs)\n",
        "\n",
        "\t\tattn_output=attn_probs @ v\n",
        "\t\tattn_output=attn_output.transpose(1,2).reshape(B,T,C)\n",
        "\n",
        "\t\toutput=self.output(attn_output)\n",
        "\t\toutput=self.dropout(output)\n",
        "\t\treturn output\n",
        "class TransformerBlock(nn.Module):\n",
        "\tdef __init__(self,cfg):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.ln1=LayerNorm(cfg['emb_dim'])\n",
        "\t\tself.attn=MultiHeadAttention(cfg)\n",
        "\t\tself.ln2=LayerNorm(cfg['emb_dim'])\n",
        "\t\tself.ff=FeedForward(cfg)\n",
        "\tdef forward(self,x):\n",
        "\t\tx=x+self.attn(self.ln1(x))\n",
        "\t\tx=x+self.ff(self.ln2(x))\n",
        "\t\treturn x\n",
        "class GPT2Model(nn.Module):\n",
        "\tdef __init__(self,cfg):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.tok_emb=nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
        "\t\tself.pos_emb =nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
        "\t\tself.drop    =nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "\t\tself.blocks=nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "\t\tself.ln_f=LayerNorm(cfg['emb_dim'])\n",
        "\t\tself.head =nn.Linear(cfg['emb_dim'])\n",
        "\tdef forward(self,idx):\n",
        "\t\tB,T=idx.size()\n",
        "\n",
        "\t\ttok_emb=self.tok_emb(idx)\n",
        "\t\tpos=torch.arrange(T,device=idx.device)\n",
        "\t\tpos_emb=self.pos_emb(pos)\n",
        "\n",
        "\t\tx=tok_emb+pos_emb\n",
        "\t\tx=self.drop(x)\n",
        "\t\tx-self.blocks(x)\n",
        "\t\tx=self.ln_f(x)\n",
        "\t\tlogits=self.head(x)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "nvbb0TlOwhh9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9SHhu8TAO_UK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}