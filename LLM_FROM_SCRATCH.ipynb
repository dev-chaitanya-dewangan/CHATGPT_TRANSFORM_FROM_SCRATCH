{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "eDFX03NRdvEu"
      ],
      "authorship_tag": "ABX9TyNPv7XhO17A+HbjG53rlrtE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dev-chaitanya-dewangan/CHATGPT_TRANSFORM_FROM_SCRATCH/blob/main/LLM_FROM_SCRATCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREP"
      ],
      "metadata": {
        "id": "TRzYCvG0OJZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0ehBrOX-MEha"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "url = \"https://www.gutenberg.org/cache/epub/76137/pg76137.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Get the text content\n",
        "full_text = response.text\n",
        "first_index = full_text.find(\"PREFACE\")\n",
        "second_index = full_text.find(\"PREFACE\", first_index + 1)\n",
        "# Preview first 500 characters\n",
        "book= full_text[second_index:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preproccesed_cleaned_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)',book)\n",
        "preporcessed_result=[item.strip() for item in preproccesed_cleaned_text if item.strip()]"
      ],
      "metadata": {
        "id": "LW2SFb8qiN9p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EXTENED WITH UNK AND ENDOF TEXT**"
      ],
      "metadata": {
        "id": "4ld_NlhPiPKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preporcessed_sorted_result=sorted(set(preporcessed_result))\n",
        "preporcessed_sorted_result.extend([\"<|endoftext|>\",\"<|unk|>\"])"
      ],
      "metadata": {
        "id": "gUkOSLJjiON5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab={token:id for id,token in enumerate(preporcessed_sorted_result)}\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JLNH4wQQO7-W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOKENIZER\n"
      ],
      "metadata": {
        "id": "jmKNB_m0YnAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self,vocab):\n",
        "    self.encoded_vocab=vocab\n",
        "    self.decoded_vocab={id:word for word,id in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    text_cleaning=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preporccesed_text=[item.strip() for item in text_cleaning if item.strip()]\n",
        "    tokenized_text = [self.encoded_vocab[i] for i in preporccesed_text]\n",
        "    return tokenized_text\n",
        "  def decode(self,tokens):\n",
        "    text=\" \".join([self.decoded_vocab[i] for i in tokens])\n",
        "    text=re.sub('\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "kgs5MmMwO_Jc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEW TOKENIZER EXTENDED WITH THE UNKOWN TEXT AND END OF TEXT\\"
      ],
      "metadata": {
        "id": "dyi2_RrFjlRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TokenizerV2:\n",
        "  def __init__(self,vocab):\n",
        "    self.encoded_vocab=vocab\n",
        "    self.decoded_vocab={id:word for word,id in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    text_cleaning=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preporccesed_text=[item.strip() for item in text_cleaning if item.strip()]\n",
        "    # tokenized_text = [self.encoded_vocab[i] for i in preporccesed_text] CHANGED HERE\n",
        "    tokenized_text = [i if i in self.encoded_vocab else \"<|unk|>\" for i in preporccesed_text]\n",
        "    return tokenized_text\n",
        "  def decode(self,tokens):\n",
        "    text=\" \".join([self.decoded_vocab[i] for i in tokens])\n",
        "    text=re.sub('\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "bLNvdygVO_MS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TokenizerV2(vocab)\n"
      ],
      "metadata": {
        "id": "r5_E2-50v4co"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "class GPTEncoder(Dataset):\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.input=[]\n",
        "    self.target=[]\n",
        "    tokenized_text=tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
        "    for i in range(0,len(tokenized_text)-max_length,stride):\n",
        "      input=tokenized_text[i:i+max_length]\n",
        "      target=tokenized_text[i+1:1+i+max_length]\n",
        "      self.input.append(torch.tensor(input))\n",
        "      self.target.append(torch.tensor(target))\n",
        "  def __len__(self):\n",
        "    return len(self.input)\n",
        "  def __getitem__(self,pos):\n",
        "    return self.input[pos],self.target[pos]\n",
        "\n",
        "def create_dataloader(txt,batch_size=4,max_length=256\n",
        "                      ,stride=128,shuffle=True\n",
        "                      ,drop_last=True,num_workers=0):\n",
        "  tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTEncoder(txt,tokenizer,max_length,stride)\n",
        "  dataloader=DataLoader(\n",
        "      dataset=dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "  return dataloader\n",
        "book= full_text[second_index:]\n",
        "batch_dataloader=create_dataloader(book,batch_size=1,max_length=12,stride=120,shuffle=False)\n",
        "data_iter=iter(batch_dataloader)\n",
        "# first=next(batch_dataloader)\n",
        "print(next(data_iter))"
      ],
      "metadata": {
        "id": "uZxmxh_tO_Oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdf8c539-4267-4ed5-def4-04c4ee376f6b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[   47, 31688, 11598,    13,   201,   198,   201,   198,   201,   198,\n",
            "           464,  1204]]), tensor([[31688, 11598,    13,   201,   198,   201,   198,   201,   198,   464,\n",
            "          1204,   286]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SIMPLIFIED SELF ATTENTION**"
      ],
      "metadata": {
        "id": "mQNgJ7O4wmHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "attenS=torch.empty(inputs.shape[0])\n",
        "query = inputs[1]\n",
        "for i,x_i in  enumerate(inputs):\n",
        "  attenS[i]=torch.dot(x_i,query)\n",
        "\n",
        "print(attenS)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "966PA7Fauzfc",
        "outputId": "a9ccbb53-18fd-4f7a-f1e8-4f25a4d98a7e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attenS=torch.softmax(attenS,dim=0)\n",
        "print(attenS.sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlz9V8uPwdHe",
        "outputId": "93549359-8818-461c-b1d6-a11ad3084ade"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(1.)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[3,5,9], # Your     (x^1)\n",
        "   [5,7,6], # journey  (x^2)\n",
        "   [7,5,4], # starts   (x^3)\n",
        "   [2,8,3], # with     (x^4)\n",
        "   [7,5,0], # one      (x^5)\n",
        "   [5,0,5]] # step     (x^6)\n",
        ",dtype=torch.float32)\n",
        "\n",
        "atten_scores=inputs @ inputs.T\n",
        "attention_weights=torch.softmax(atten_scores,dim=-1)\n",
        "context=attention_weights @ atten_scores\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8CPLX5X6O_Re"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GELU FUNCTION**"
      ],
      "metadata": {
        "id": "YBDGqBsao9MN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "sf5zjo0zpQIt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class Gelu(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return 0.5 * x * (torch.tanh(1+torch.sqrt(2.0/torch.pi) *(x +0.044715 * torch.pow(x,3))))"
      ],
      "metadata": {
        "id": "tqgS0Cbqnev4"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,input_size):\n",
        "    super().__init__()\n",
        "    layer_neurons_size= 4 * GPT_CONFIG_124M['emb_dim']\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(input_size,768,layer_neurons_size),\n",
        "        Gelu(),\n",
        "        nn.Linear(layer_neurons_size,768)\n",
        "\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "3tcGF-fGngQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J3bbGytUngNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVg34DtangJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Zf0byWwngGc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kPkxC95-ngDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ww9_DqukngBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f-FCZvn4nf_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "duaQA4YMnf9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TY-t0JQ6nf6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fi1zIQV-nf30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SVSVYir9nfwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LTJcaSkVnfjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FULL CHATGPT MODEL IMPLEMENTED**"
      ],
      "metadata": {
        "id": "eDFX03NRdvEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #CONFIG FOR GPT2 144M\n",
        "# GPT2_CONFIG_144M = {\n",
        "\n",
        "# \t\"vocab_size\":50257,\n",
        "# \t\"n_heads\":12,\n",
        "# \t\"n_layers\":12,\n",
        "# \t\"emb_dim\":768,\n",
        "# \t\"context_length\":1024,\n",
        "# \t\"drop_rate\":0.1,\n",
        "# \t\"ff_dim\" :3072\n",
        "# }\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# class LayerNorm(nn.module):\n",
        "# \tdef __init__(self,dim,eps=1e-5):\n",
        "# \t\tsuper().__init__()\n",
        "# \t\tself.wieghts =nn.Parameter(torch.ones(dim))\n",
        "# \t\tself.bias =nn.Parameter(torch.zeros(dim))\n",
        "# \t\tself.eps=eps\n",
        "# \tdef forward(self,x):\n",
        "# \t\tmean   =x.mean(-1,keepdim=True)\n",
        "# \t\tvar    =x.var(-1,keepdim=True,unbiased=False)\n",
        "# \t\tx_norm =(x-mean)/torch.sqrt(var+self.eps)\n",
        "# \t\treturn self.weights*x_norm+self.bias\n",
        "# class FeedForward(nn.Module):\n",
        "\n",
        "# \tdef __init__(self,cfg):\n",
        "# \t\tsuper().__init__()\n",
        "# \t\tself.net=nn.Sequential(\n",
        "# \t\t\tnn.Linear(cfg['emb_dim'],cfg['ff_dim']),\n",
        "# \t\t\tnn.GELU(),\n",
        "# \t\t\tnn.Linear(cfg['ff_dim'],cfg['emb_dim']),\n",
        "# \t\t\tnn.Dropout(cfg['drop_rate'])\n",
        "# \t\t\t)\n",
        "# \tdef forward(self,x):\n",
        "# \t\treturn self.net(x)\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "# \tdef __init__(self,cfg):\n",
        "# \t\tself.n_heads=cfg['n_heads']\n",
        "# \t\tself.emb_dim=cfg['emb_dim']\n",
        "# \t\tself.head_dim=self.emb_dim/self.n_heads\n",
        "# \t\tassert self.head_dim*self.n_heads == self.emb_dim\n",
        "\n",
        "# \t\tself.context_input=nn.Linear(self.emb_dim,3*self.emb_dim)\n",
        "# \t\tself.output       =nn.Linear(self.emb_dim,self.emb_dim)\n",
        "# \t\tself.dropout      =nn.Droptout(cfg['dropt_rate'])\n",
        "\n",
        "# \t\tself.register_buffer(\"mask\",torch.tril(torch.ones(cfg[\"context_length\"],cfg[\"context_length\"])).unsqueeze(0).unsqueeze(0))\n",
        "# \tdef forward(self,x):\n",
        "# \t\tB,T,C=x.size()\n",
        "\n",
        "# \t\tcontext_input=self.context_input(x)\n",
        "# \t\tcontext_input=context_input.reshape(B,T,3,self.n_head,self.head_dim)\n",
        "# \t\tcontext_input=context_input.permute(2,0,3,1,4)\n",
        "# \t\tq,k,v=context_input[0],context_input[1],context_input[2]\n",
        "\n",
        "# \t\tattn_scores=(q@k.transpose(-2,-1) / (self.head_dim**o.5))\n",
        "# \t\tattn_probs =F.softmax(attn_scores,dim=-1)\n",
        "# \t\tattn_probs =self.dropout(attn_probs)\n",
        "\n",
        "# \t\tattn_output=attn_probs @ v\n",
        "# \t\tattn_output=attn_output.transpose(1,2).reshape(B,T,C)\n",
        "\n",
        "# \t\toutput=self.output(attn_output)\n",
        "# \t\toutput=self.dropout(output)\n",
        "# \t\treturn output\n",
        "# class TransformerBlock(nn.Module):\n",
        "# \tdef __init__(self,cfg):\n",
        "# \t\tsuper().__init__()\n",
        "# \t\tself.ln1=LayerNorm(cfg['emb_dim'])\n",
        "# \t\tself.attn=MultiHeadAttention(cfg)\n",
        "# \t\tself.ln2=LayerNorm(cfg['emb_dim'])\n",
        "# \t\tself.ff=FeedForward(cfg)\n",
        "# \tdef forward(self,x):\n",
        "# \t\tx=x+self.attn(self.ln1(x))\n",
        "# \t\tx=x+self.ff(self.ln2(x))\n",
        "# \t\treturn x\n",
        "# class GPT2Model(nn.Module):\n",
        "# \tdef __init__(self,cfg):\n",
        "# \t\tsuper().__init__()\n",
        "# \t\tself.tok_emb=nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
        "# \t\tself.pos_emb =nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
        "# \t\tself.drop    =nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "# \t\tself.blocks=nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "# \t\tself.ln_f=LayerNorm(cfg['emb_dim'])\n",
        "# \t\tself.head =nn.Linear(cfg['emb_dim'])\n",
        "# \tdef forward(self,idx):\n",
        "# \t\tB,T=idx.size()\n",
        "\n",
        "# \t\ttok_emb=self.tok_emb(idx)\n",
        "# \t\tpos=torch.arrange(T,device=idx.device)\n",
        "# \t\tpos_emb=self.pos_emb(pos)\n",
        "\n",
        "# \t\tx=tok_emb+pos_emb\n",
        "# \t\tx=self.drop(x)\n",
        "# \t\tx-self.blocks(x)\n",
        "# \t\tx=self.ln_f(x)\n",
        "# \t\tlogits=self.head(x)"
      ],
      "metadata": {
        "id": "nvbb0TlOwhh9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "63f49939-0f11-4fa8-823d-5a9a56f332ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-12-957674d0f928>, line 59)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-957674d0f928>\"\u001b[0;36m, line \u001b[0;32m59\u001b[0m\n\u001b[0;31m    attn_scores=(q@k.transpose(-2,-1) / (self.head_dim**o.5))\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9SHhu8TAO_UK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}