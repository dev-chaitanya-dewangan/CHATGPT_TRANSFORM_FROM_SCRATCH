{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "TRzYCvG0OJZt",
        "4ld_NlhPiPKo",
        "jmKNB_m0YnAW",
        "dyi2_RrFjlRI",
        "mQNgJ7O4wmHZ",
        "YBDGqBsao9MN",
        "g2yKfbDgsY_I",
        "eDFX03NRdvEu"
      ],
      "authorship_tag": "ABX9TyMWmWEIrQkyMI4x3PmlfHNK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dev-chaitanya-dewangan/CHATGPT_TRANSFORM_FROM_SCRATCH/blob/main/LLM_FROM_SCRATCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREP"
      ],
      "metadata": {
        "id": "TRzYCvG0OJZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "0ehBrOX-MEha"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "url = \"https://www.gutenberg.org/cache/epub/76137/pg76137.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Get the text content\n",
        "full_text = response.text\n",
        "first_index = full_text.find(\"PREFACE\")\n",
        "second_index = full_text.find(\"PREFACE\", first_index + 1)\n",
        "# Preview first 500 characters\n",
        "book= full_text[second_index:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preproccesed_cleaned_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)',book)\n",
        "preporcessed_result=[item.strip() for item in preproccesed_cleaned_text if item.strip()]"
      ],
      "metadata": {
        "id": "LW2SFb8qiN9p"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EXTENED WITH UNK AND ENDOF TEXT**"
      ],
      "metadata": {
        "id": "4ld_NlhPiPKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preporcessed_sorted_result=sorted(set(preporcessed_result))\n",
        "preporcessed_sorted_result.extend([\"<|endoftext|>\",\"<|unk|>\"])"
      ],
      "metadata": {
        "id": "gUkOSLJjiON5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab={token:id for id,token in enumerate(preporcessed_sorted_result)}\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JLNH4wQQO7-W"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOKENIZER\n"
      ],
      "metadata": {
        "id": "jmKNB_m0YnAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self,vocab):\n",
        "    self.encoded_vocab=vocab\n",
        "    self.decoded_vocab={id:word for word,id in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    text_cleaning=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preporccesed_text=[item.strip() for item in text_cleaning if item.strip()]\n",
        "    tokenized_text = [self.encoded_vocab[i] for i in preporccesed_text]\n",
        "    return tokenized_text\n",
        "  def decode(self,tokens):\n",
        "    text=\" \".join([self.decoded_vocab[i] for i in tokens])\n",
        "    text=re.sub('\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "kgs5MmMwO_Jc"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEW TOKENIZER EXTENDED WITH THE UNKOWN TEXT AND END OF TEXT\\"
      ],
      "metadata": {
        "id": "dyi2_RrFjlRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TokenizerV2:\n",
        "  def __init__(self,vocab):\n",
        "    self.encoded_vocab=vocab\n",
        "    self.decoded_vocab={id:word for word,id in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    text_cleaning=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preporccesed_text=[item.strip() for item in text_cleaning if item.strip()]\n",
        "    # tokenized_text = [self.encoded_vocab[i] for i in preporccesed_text] CHANGED HERE\n",
        "    tokenized_text = [i if i in self.encoded_vocab else \"<|unk|>\" for i in preporccesed_text]\n",
        "    return tokenized_text\n",
        "  def decode(self,tokens):\n",
        "    text=\" \".join([self.decoded_vocab[i] for i in tokens])\n",
        "    text=re.sub('\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "bLNvdygVO_MS"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer=TokenizerV2(vocab)\n"
      ],
      "metadata": {
        "id": "r5_E2-50v4co"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "import torch\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "class GPTEncoder(Dataset):\n",
        "  def __init__(self,txt,tokenizer,max_length,stride):\n",
        "    self.input=[]\n",
        "    self.target=[]\n",
        "    tokenized_text=tokenizer.encode(txt,allowed_special={\"<|endoftext|>\"})\n",
        "    for i in range(0,len(tokenized_text)-max_length,stride):\n",
        "      input=tokenized_text[i:i+max_length]\n",
        "      target=tokenized_text[i+1:1+i+max_length]\n",
        "      self.input.append(torch.tensor(input))\n",
        "      self.target.append(torch.tensor(target))\n",
        "  def __len__(self):\n",
        "    return len(self.input)\n",
        "  def __getitem__(self,pos):\n",
        "    return self.input[pos],self.target[pos]\n",
        "\n",
        "def create_dataloader(txt,batch_size=4,max_length=256\n",
        "                      ,stride=128,shuffle=True\n",
        "                      ,drop_last=True,num_workers=0):\n",
        "  tokenizer=tiktoken.get_encoding(\"gpt2\")\n",
        "  dataset = GPTEncoder(txt,tokenizer,max_length,stride)\n",
        "  dataloader=DataLoader(\n",
        "      dataset=dataset,\n",
        "      batch_size=batch_size,\n",
        "      shuffle=shuffle,\n",
        "      drop_last=drop_last,\n",
        "      num_workers=num_workers\n",
        "  )\n",
        "  return dataloader\n",
        "book= full_text[second_index:]\n",
        "batch_dataloader=create_dataloader(book,batch_size=1,max_length=12,stride=120,shuffle=False)\n",
        "data_iter=iter(batch_dataloader)\n",
        "# first=next(batch_dataloader)\n",
        "print(next(data_iter))"
      ],
      "metadata": {
        "id": "uZxmxh_tO_Oq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbbe4645-56f1-44b2-e596-ba469c0fa74b"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[tensor([[   47, 31688, 11598,    13,   201,   198,   201,   198,   201,   198,\n",
            "           464,  1204]]), tensor([[31688, 11598,    13,   201,   198,   201,   198,   201,   198,   464,\n",
            "          1204,   286]])]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SIMPLIFIED SELF ATTENTION**"
      ],
      "metadata": {
        "id": "mQNgJ7O4wmHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = torch.tensor(\n",
        "  [[3,5,9], # Your     (x^1)\n",
        "   [5,7,6], # journey  (x^2)\n",
        "   [7,5,4], # starts   (x^3)\n",
        "   [2,8,3], # with     (x^4)\n",
        "   [7,5,0], # one      (x^5)\n",
        "   [5,0,5]] # step     (x^6)\n",
        ",dtype=torch.float32)\n",
        "\n",
        "atten_scores=inputs @ inputs.T\n",
        "attention_weights=torch.softmax(atten_scores,dim=-1)\n",
        "context=attention_weights @ atten_scores\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8CPLX5X6O_Re"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M50EtrJlvXUG"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GPT _CONFIG**"
      ],
      "metadata": {
        "id": "zYhwuvj4vsU8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "vJFa3vTyvrNp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **MULTIHEAD ATTENTION**"
      ],
      "metadata": {
        "id": "GS8p7uxQsmEo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class MHAttention(nn.Module):\n",
        "  def __init__(self,d_in,d_out,cfg):\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    context_length=cfg['context_length']\n",
        "    emb_size=cfg['emb_dim']\n",
        "    qkv_bias=cfg['qkv_bias']\n",
        "\n",
        "    self.d_out=d_out\n",
        "    self.num_head=cfg['n_heads']\n",
        "    self.head_dim=d_out//self.num_head\n",
        "    assert d_out % self.num_head == 0, \"d_out must be divisible by number of heads\"\n",
        "\n",
        "    self.W_key=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_query=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "    self.W_value=nn.Linear(d_in,d_out,bias=qkv_bias)\n",
        "\n",
        "    self.out_projection=nn.Linear(d_out,d_out)\n",
        "    self.dropout=nn.Dropout(cfg['drop_rate'])\n",
        "    self.register_buffer(\"mask\",torch.triu(torch.ones(context_length,context_length)))\n",
        "\n",
        "  def forward(self,x):\n",
        "    b,num_token,d_in=x.shape\n",
        "\n",
        "    key    =  self.W_key(x)\n",
        "    query  =  self.W_query(x)\n",
        "    value  =  self.W_value(x)\n",
        "\n",
        "    key     = key.view(     b , num_token ,self.num_head , self.head_dim)\n",
        "    query   = query.view(   b , num_token ,self.num_head , self.head_dim)\n",
        "    value   = value.view(   b , num_token ,self.num_head , self.head_dim)\n",
        "\n",
        "    key     = key.transpose(1,2)\n",
        "    query   = query.transpose(1,2)\n",
        "    value   = value.transpose(1,2)\n",
        "\n",
        "    attention_score  = query@ key.transpose(2,3)\n",
        "    mask_bool=self.mask.bool()[:num_token,:num_token]\n",
        "    attention_score.masked_fill(mask_bool,-torch.inf)\n",
        "\n",
        "    attention_weights     =torch.softmax(attention_score/key.shape[-1] * 0.5,dim=-1)\n",
        "    attention_weights  =self.dropout(attention_weights)\n",
        "\n",
        "    context_vec = (attention_weights @ value).transpose(1,2)\n",
        "\n",
        "    context_vec = context_vec.contiguous().view(b,num_token,self.d_out)\n",
        "    context_vec = self.out_projection(context_vec)\n",
        "\n",
        "    return context_vec\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "mha = MHAttention(768,768,GPT_CONFIG_124M)\n",
        "x = torch.randn(2, 1024, 768)\n",
        "mha(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dz56YYNesrIS",
        "outputId": "a8383e0e-c7e5-4e12-db63-e3fc1d4d03c6"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 3.0145e-02, -1.1210e-02, -9.6214e-03,  ..., -4.1526e-02,\n",
              "           1.5352e-02, -5.8658e-03],\n",
              "         [ 2.6093e-02, -1.4181e-02, -5.6811e-03,  ..., -4.4064e-02,\n",
              "           1.7080e-02,  5.4527e-03],\n",
              "         [ 2.9279e-02, -1.5545e-02, -9.7792e-03,  ..., -3.8536e-02,\n",
              "           1.3967e-02, -8.0223e-03],\n",
              "         ...,\n",
              "         [ 2.4948e-02, -1.2493e-02, -8.1560e-03,  ..., -4.9425e-02,\n",
              "           1.5062e-02, -8.5777e-03],\n",
              "         [ 2.3182e-02, -1.1238e-02, -9.4325e-03,  ..., -4.4405e-02,\n",
              "           2.0213e-02, -1.3341e-03],\n",
              "         [ 3.0325e-02, -1.1326e-02, -1.2362e-02,  ..., -3.8602e-02,\n",
              "           2.1890e-02,  1.0033e-03]],\n",
              "\n",
              "        [[ 2.0443e-02, -4.6763e-03,  3.0779e-03,  ..., -4.2905e-02,\n",
              "           3.6219e-02,  6.9410e-04],\n",
              "         [ 2.0991e-02,  2.3195e-03,  3.8737e-03,  ..., -5.1898e-02,\n",
              "           3.1167e-02,  5.1967e-03],\n",
              "         [ 2.0206e-02, -1.5335e-03,  9.7019e-03,  ..., -4.6209e-02,\n",
              "           3.3122e-02, -3.6337e-05],\n",
              "         ...,\n",
              "         [ 1.3722e-02, -6.0607e-03,  7.9633e-03,  ..., -5.0176e-02,\n",
              "           2.6894e-02,  1.2530e-03],\n",
              "         [ 1.1603e-02,  3.5461e-03,  7.9798e-03,  ..., -4.8463e-02,\n",
              "           3.1760e-02,  4.6762e-03],\n",
              "         [ 1.9326e-02, -6.0845e-03,  1.1908e-02,  ..., -4.3125e-02,\n",
              "           4.2754e-02,  2.7013e-03]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ],
      "metadata": {
        "id": "ycBuy6_r3Bql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 338
        },
        "id": "vfWLaNepsqxb",
        "outputId": "a6596425-7133-465e-8454-89b48e97e1fe"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "shape '[2, 3, 12, 0]' is invalid for input of size 36",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-7b9cc094ca2f>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMHAttention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mGPT_CONFIG_124M\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-60-6d883962fdd4>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mvalue\u001b[0m  \u001b[0;34m=\u001b[0m  \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mkey\u001b[0m     \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m     \u001b[0mb\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnum_token\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_head\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mquery\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnum_token\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_head\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mvalue\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m   \u001b[0mb\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mnum_token\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_head\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: shape '[2, 3, 12, 0]' is invalid for input of size 36"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GELU FUNCTION**"
      ],
      "metadata": {
        "id": "YBDGqBsao9MN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sf5zjo0zpQIt"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class Gelu(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "  def forward(self,x):\n",
        "    return 0.5 * x * (torch.tanh(1+torch.sqrt(torch.tensor(2.0/torch.pi)) *(x +0.044715 * torch.pow(x,3))))"
      ],
      "metadata": {
        "id": "tqgS0Cbqnev4"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "  def __init__(self,cfg):\n",
        "    super().__init__()\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Linear(cfg['emb_dim'],4*cfg['emb_dim']),\n",
        "        Gelu(),\n",
        "        nn.Linear(4*cfg['emb_dim'],cfg['emb_dim'])\n",
        "\n",
        "    )\n",
        "  def forward(self,x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "3tcGF-fGngQE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = torch.rand(12, 31, 768) #A\n",
        "out=FeedForward(GPT_CONFIG_124M)\n",
        "out=out(x)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "J3bbGytUngNM"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **SHORTCUT**"
      ],
      "metadata": {
        "id": "g2yKfbDgsY_I"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zVg34DtangJD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cQq0KW0osGyF"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Zf0byWwngGc"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kPkxC95-ngDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ww9_DqukngBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f-FCZvn4nf_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "duaQA4YMnf9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TY-t0JQ6nf6X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fi1zIQV-nf30"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SVSVYir9nfwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LTJcaSkVnfjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **FULL CHATGPT MODEL IMPLEMENTED**"
      ],
      "metadata": {
        "id": "eDFX03NRdvEu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# #CONFIG FOR GPT2 144M\n",
        "# GPT2_CONFIG_144M = {\n",
        "\n",
        "# \t\"vocab_size\":50257,\n",
        "# \t\"n_heads\":12,\n",
        "# \t\"n_layers\":12,\n",
        "# \t\"emb_dim\":768,\n",
        "# \t\"context_length\":1024,\n",
        "# \t\"drop_rate\":0.1,\n",
        "# \t\"ff_dim\" :3072\n",
        "# }\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.nn.functional as F\n",
        "# class LayerNorm(nn.module):\n",
        "# \tdef __init__(self,dim,eps=1e-5):\n",
        "# \t\tsuper().__init__()\n",
        "# \t\tself.wieghts =nn.Parameter(torch.ones(dim))\n",
        "# \t\tself.bias =nn.Parameter(torch.zeros(dim))\n",
        "# \t\tself.eps=eps\n",
        "# \tdef forward(self,x):\n",
        "# \t\tmean   =x.mean(-1,keepdim=True)\n",
        "# \t\tvar    =x.var(-1,keepdim=True,unbiased=False)\n",
        "# \t\tx_norm =(x-mean)/torch.sqrt(var+self.eps)\n",
        "# \t\treturn self.weights*x_norm+self.bias\n",
        "# class FeedForward(nn.Module):\n",
        "\n",
        "# \tdef __init__(self,cfg):\n",
        "# \t\tsuper().__init__()\n",
        "# \t\tself.net=nn.Sequential(\n",
        "# \t\t\tnn.Linear(cfg['emb_dim'],cfg['ff_dim']),\n",
        "# \t\t\tnn.GELU(),\n",
        "# \t\t\tnn.Linear(cfg['ff_dim'],cfg['emb_dim']),\n",
        "# \t\t\tnn.Dropout(cfg['drop_rate'])\n",
        "# \t\t\t)\n",
        "# \tdef forward(self,x):\n",
        "# \t\treturn self.net(x)\n",
        "# class MultiHeadAttention(nn.Module):\n",
        "# \tdef __init__(self,cfg):\n",
        "# \t\tself.n_heads=cfg['n_heads']\n",
        "# \t\tself.emb_dim=cfg['emb_dim']\n",
        "# \t\tself.head_dim=self.emb_dim/self.n_heads\n",
        "# \t\tassert self.head_dim*self.n_heads == self.emb_dim\n",
        "\n",
        "# \t\tself.context_input=nn.Linear(self.emb_dim,3*self.emb_dim)\n",
        "# \t\tself.output       =nn.Linear(self.emb_dim,self.emb_dim)\n",
        "# \t\tself.dropout      =nn.Droptout(cfg['dropt_rate'])\n",
        "\n",
        "# \t\tself.register_buffer(\"mask\",torch.tril(torch.ones(cfg[\"context_length\"],cfg[\"context_length\"])).unsqueeze(0).unsqueeze(0))\n",
        "# \tdef forward(self,x):\n",
        "# \t\tB,T,C=x.size()\n",
        "\n",
        "# \t\tcontext_input=self.context_input(x)\n",
        "# \t\tcontext_input=context_input.reshape(B,T,3,self.n_head,self.head_dim)\n",
        "# \t\tcontext_input=context_input.permute(2,0,3,1,4)\n",
        "# \t\tq,k,v=context_input[0],context_input[1],context_input[2]\n",
        "\n",
        "# \t\tattn_scores=(q@k.transpose(-2,-1) / (self.head_dim**o.5))\n",
        "# \t\tattn_probs =F.softmax(attn_scores,dim=-1)\n",
        "# \t\tattn_probs =self.dropout(attn_probs)\n",
        "\n",
        "# \t\tattn_output=attn_probs @ v\n",
        "# \t\tattn_output=attn_output.transpose(1,2).reshape(B,T,C)\n",
        "\n",
        "# \t\toutput=self.output(attn_output)\n",
        "# \t\toutput=self.dropout(output)\n",
        "# \t\treturn output\n",
        "# class TransformerBlock(nn.Module):\n",
        "# \tdef __init__(self,cfg):\n",
        "# \t\tsuper().__init__()\n",
        "# \t\tself.ln1=LayerNorm(cfg['emb_dim'])\n",
        "# \t\tself.attn=MultiHeadAttention(cfg)\n",
        "# \t\tself.ln2=LayerNorm(cfg['emb_dim'])\n",
        "# \t\tself.ff=FeedForward(cfg)\n",
        "# \tdef forward(self,x):\n",
        "# \t\tx=x+self.attn(self.ln1(x))\n",
        "# \t\tx=x+self.ff(self.ln2(x))\n",
        "# \t\treturn x\n",
        "# class GPT2Model(nn.Module):\n",
        "# \tdef __init__(self,cfg):\n",
        "# \t\tsuper().__init__()\n",
        "# \t\tself.tok_emb=nn.Embedding(cfg['vocab_size'],cfg['emb_dim'])\n",
        "# \t\tself.pos_emb =nn.Embedding(cfg['context_length'],cfg['emb_dim'])\n",
        "# \t\tself.drop    =nn.Dropout(cfg['drop_rate'])\n",
        "\n",
        "# \t\tself.blocks=nn.Sequential(*[TransformerBlock(cfg) for _ in range(cfg['n_layers'])])\n",
        "# \t\tself.ln_f=LayerNorm(cfg['emb_dim'])\n",
        "# \t\tself.head =nn.Linear(cfg['emb_dim'])\n",
        "# \tdef forward(self,idx):\n",
        "# \t\tB,T=idx.size()\n",
        "\n",
        "# \t\ttok_emb=self.tok_emb(idx)\n",
        "# \t\tpos=torch.arrange(T,device=idx.device)\n",
        "# \t\tpos_emb=self.pos_emb(pos)\n",
        "\n",
        "# \t\tx=tok_emb+pos_emb\n",
        "# \t\tx=self.drop(x)\n",
        "# \t\tx-self.blocks(x)\n",
        "# \t\tx=self.ln_f(x)\n",
        "# \t\tlogits=self.head(x)"
      ],
      "metadata": {
        "id": "nvbb0TlOwhh9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 111
        },
        "outputId": "63f49939-0f11-4fa8-823d-5a9a56f332ad"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (<ipython-input-12-957674d0f928>, line 59)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-957674d0f928>\"\u001b[0;36m, line \u001b[0;32m59\u001b[0m\n\u001b[0;31m    attn_scores=(q@k.transpose(-2,-1) / (self.head_dim**o.5))\u001b[0m\n\u001b[0m                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9SHhu8TAO_UK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}