{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMpbTMOuo++E1AAk0ZAxWlq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dev-chaitanya-dewangan/CHATGPT_TRANSFORM_FROM_SCRATCH/blob/main/LLM_FROM_SCRATCH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DATA PREP"
      ],
      "metadata": {
        "id": "TRzYCvG0OJZt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "0ehBrOX-MEha"
      },
      "outputs": [],
      "source": [
        "import requests\n",
        "import re\n",
        "url = \"https://www.gutenberg.org/cache/epub/76137/pg76137.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "# Get the text content\n",
        "full_text = response.text\n",
        "first_index = full_text.find(\"PREFACE\")\n",
        "second_index = full_text.find(\"PREFACE\", first_index + 1)\n",
        "# Preview first 500 characters\n",
        "book= full_text[second_index:]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preproccesed_cleaned_text = re.split(r'([,.:;?_!\"()\\']|--|\\s)',book)\n",
        "preporcessed_result=[item.strip() for item in preproccesed_cleaned_text if item.strip()]"
      ],
      "metadata": {
        "id": "LW2SFb8qiN9p"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EXTENED WITH UNK AND ENDOF TEXT**"
      ],
      "metadata": {
        "id": "4ld_NlhPiPKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "preporcessed_sorted_result=sorted(set(preporcessed_result))\n",
        "preporcessed_sorted_result.extend([\"<|endoftext|>\",\"<|unk|>\"])"
      ],
      "metadata": {
        "id": "gUkOSLJjiON5"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab={token:id for id,token in enumerate(preporcessed_sorted_result)}\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JLNH4wQQO7-W"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0eZL80RecGa",
        "outputId": "25caef87-9245-4f35-9cd2-3e6e4bafbc1a"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9502\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TOKENIZER\n"
      ],
      "metadata": {
        "id": "jmKNB_m0YnAW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer:\n",
        "  def __init__(self,vocab):\n",
        "    self.encoded_vocab=vocab\n",
        "    self.decoded_vocab={id:word for word,id in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    text_cleaning=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preporccesed_text=[item.strip() for item in text_cleaning if item.strip()]\n",
        "    tokenized_text = [self.encoded_vocab[i] for i in preporccesed_text]\n",
        "    return tokenized_text\n",
        "  def decode(self,tokens):\n",
        "    text=\" \".join([self.decoded_vocab[i] for i in tokens])\n",
        "    text=re.sub('\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text\n",
        "tokenizer=Tokenizer(vocab)\n",
        "text=\"A\"\n",
        "tokenizer.decode([0]),tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgs5MmMwO_Jc",
        "outputId": "ddc61a45-9dde-400c-add3-03aa87018923"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('!', [471])"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NEW TOKENIZER EXTENDED WITH THE UNKOWN TEXT AND END OF TEXT\\"
      ],
      "metadata": {
        "id": "dyi2_RrFjlRI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TokenizerV2:\n",
        "  def __init__(self,vocab):\n",
        "    self.encoded_vocab=vocab\n",
        "    self.decoded_vocab={id:word for word,id in vocab.items()}\n",
        "\n",
        "  def encode(self,text):\n",
        "    text_cleaning=re.split(r'([,.:;?_!\"()\\']|--|\\s)',text)\n",
        "    preporccesed_text=[item.strip() for item in text_cleaning if item.strip()]\n",
        "    # tokenized_text = [self.encoded_vocab[i] for i in preporccesed_text] CHANGED HERE\n",
        "    tokenized_text = [i if i in self.encoded_vocab else \"<|unk|>\" for i in preporccesed_text]\n",
        "    return tokenized_text\n",
        "  def decode(self,tokens):\n",
        "    text=\" \".join([self.decoded_vocab[i] for i in tokens])\n",
        "    text=re.sub('\\s+([,.?!\"()\\'])',r'\\1',text)\n",
        "    return text\n",
        "tokenizer=TokenizerV2(vocab)\n",
        "text=\"aaa\"\n",
        "tokenizer.encode(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLNvdygVO_MS",
        "outputId": "93ca2ccb-a886-4807-a966-b2e4e492820e"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['<|unk|>']"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uZxmxh_tO_Oq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8CPLX5X6O_Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9SHhu8TAO_UK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}